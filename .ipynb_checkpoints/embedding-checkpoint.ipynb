{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Embedding Generation Script for scRNA-seq and scATAC-seq Data\n",
    "### This script generates embeddings for single-cell RNA-seq (scRNA-seq) and ATAC-seq (scATAC-seq) data using a pre-trained transformer model. It processes input datasets, loads a pre-trained model, and produces latent representations that can be used for downstream analysis tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Imported Packages, Initial Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/share/t_lgl/scFM/shared/Final_models_250514/SCARF/conda_scarf/lib/python3.12/site-packages/transformers/utils/import_utils.py:616: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import load_from_disk, concatenate_datasets, Dataset\n",
    "from transformers.training_args import TrainingArguments\n",
    "from transformers import BertConfig\n",
    "\n",
    "from scarf.contrastive_model import TotalModel_downstream\n",
    "from scarf.pretrainer_modified import Pretrainer\n",
    "from scarf.data_collator_modified import DataCollatorForLanguageModeling_Inference\n",
    "from scarf.pretrainer_modified import PreCollator\n",
    "\n",
    "from scarf.utils import load_model_with_index\n",
    "from tqdm import tqdm\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"OMPI_MCA_opal_cuda_support\"] = \"true\"\n",
    "os.environ[\"CONDA_OVERRIDE_GLIBC\"] = \"2.56\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. Input Parameters and Output Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class NotebookSettings:\n",
    "    def __init__(self,checkpoint_path,dset_path,h5ad_dset_path,dset_name,embed_type,has_label,modality):\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.dset_path =dset_path\n",
    "        self.h5ad_dset_path = h5ad_dset_path\n",
    "        self.dset_name = dset_name\n",
    "        self.embed_type = embed_type\n",
    "        self.has_label = has_label\n",
    "        self.modality = modality\n",
    "args = NotebookSettings(\n",
    "        checkpoint_path = './checkpoint-271755',\n",
    "        dset_path = '/public/share/t_lgl/scFM/evaluation/data_set/processed_data/09_hPBMC_10k_scGLUE_10xDemo/',\n",
    "        h5ad_dset_path =\"/public/share/t_lgl/scFM/evaluation/data_set/raw_data/09_hPBMC_10k_scGLUE_10xDemo/\",\n",
    "        dset_name = \"hPBMC_10k_scGLUE_10xDemo\",\n",
    "        embed_type = \"RNA_and_ATAC\", # ÂèØÈÄâÈ°πÔºöRNAÔºåATACÔºåRNA_and_ATAC\n",
    "        has_label = True,\n",
    "        modality = 0,\n",
    ")\n",
    "checkpoint_path = args.checkpoint_path\n",
    "dset_name = args.dset_name\n",
    "data = load_from_disk(args.dset_path)\n",
    "data_len = data.num_rows\n",
    "data_modality = Dataset.from_dict({\"modality\": [args.modality] * data_len})\n",
    "dataset = concatenate_datasets([data, data_modality], axis=1)\n",
    "sorted_len = [32000] * data_len\n",
    "\n",
    "# define output directory path\n",
    "output_path = f'./get_embeds'\n",
    "run_name = f\"/{checkpoint_path.split('/')[-2].split('__')[-1]}_{checkpoint_path.split('/')[-1]}_temp\"  \n",
    "output_dir = output_path + run_name\n",
    "os.makedirs(output_dir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3. Prior knowledge\n",
    "This code loads several prior knowledge components:\n",
    "\n",
    "Token dictionary mapping genes to tokens\n",
    "\n",
    "Homologous gene indices between species\n",
    "\n",
    "Peak IDF (Inverse Document Frequency) values\n",
    "\n",
    "Motif matrix for peak regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dict_dir = './prior_data'\n",
    "token_dict_path = f'{dict_dir}/hm_ENSG2token_dict.pickle'\n",
    "with open(token_dict_path, \"rb\") as fp:\n",
    "    token_dictionary = pickle.load(fp)\n",
    "\n",
    "priors = {}\n",
    "priors['peak_idf'] = np.load(f'{dict_dir}/peakToken_idf.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4. Model Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLMWithRNA has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now using our modified model!!!\n",
      "Now using our modified model!!!\n",
      "Now using our modified model!!!\n",
      "Now using our modified model!!!\n",
      "Now using our modified model!!!\n",
      "Now using our modified model!!!\n",
      "Now using our modified model!!!\n",
      "Now using our modified model!!!\n",
      "Now using our modified model!!!\n",
      "Now using our modified model!!!\n",
      "{'CTA_num': 164, 'R2R_rec': 'poisson', 'attention_probs_dropout_prob': 0.02, 'attn_cfg': {'num_heads': 8}, 'attn_layer_idx': [6], 'd_intermediate': 0, 'd_model': 512, 'encoder_type': 'mamba', 'expand': 2, 'fused_add_norm': True, 'head_dim': 64, 'hidden_act': 'silu', 'hidden_dropout_prob': 0.02, 'hidden_size': 512, 'inidtializer_range': 0.02, 'intermediate_size': 1024, 'kl_dim_rna': 256, 'layer_norm_eps': 1e-12, 'mamba_version': 'v_manual', 'max_position_embeddings': 2049, 'n_groups': 8, 'n_layer': 12, 'num_attention_heads': 4, 'num_heads': 16, 'num_hidden_layers': 12, 'pad_token_id': 0, 'pad_vocab_size_multiple': 16, 'residual_in_fp32': True, 'rms_norm': True, 'rna_max_input_size': 4096, 'rna_mlm_probability': 0.15, 'rna_pos_max_size': 8192, 'sel_gene_aug': 1, 'ssm_cfg': {'layer': 'Mamba2'}, 'state_size': 128, 'tie_embeddings': True, 'use_CTA': False, 'use_cls_gene_align': True, 'use_random_sample': False, 'use_rna_cl': False, 'vocab_size': 40990}\n",
      "Now using ATAC model!!!\n",
      "Now using ATAC model!!!\n",
      "Now using ATAC model!!!\n",
      "Now using ATAC model!!!\n",
      "Now using ATAC model!!!\n",
      "Now using ATAC model!!!\n",
      "Now using ATAC model!!!\n",
      "Now using ATAC model!!!\n",
      "Now using ATAC model!!!\n",
      "Now using ATAC model!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotalModel_downstream(\n",
      "  (encoder_rna): BertForMaskedLMWithRNA(\n",
      "    (bert): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(40990, 512, padding_idx=0)\n",
      "        (values_embeddings): ContinuousValueEncoder(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (concat_embeddings): Sequential(\n",
      "          (cat_fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          (cat_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (cat_gelu): QuickGELU()\n",
      "          (cat_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (position_embeddings): Embedding(8195, 512)\n",
      "        (token_type_embeddings): Embedding(2, 512)\n",
      "        (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (species_embeddings): Embedding(2, 512)\n",
      "        (modality_embeddings): Embedding(2, 512)\n",
      "      )\n",
      "      (encoder): MambaEncoder(\n",
      "        (backbone): MixerModel_new(\n",
      "          (layers): ModuleList(\n",
      "            (0-5): 6 x Block(\n",
      "              (norm): RMSNorm()\n",
      "              (mixer): Mamba2(\n",
      "                (in_proj): Linear(in_features=512, out_features=2320, bias=False)\n",
      "                (conv1d): Conv1d(1280, 1280, kernel_size=(4,), stride=(1,), padding=(3,), groups=1280)\n",
      "                (act): SiLU()\n",
      "                (norm): RMSNorm()\n",
      "                (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (6): Block(\n",
      "              (norm): RMSNorm()\n",
      "              (mixer): MHA(\n",
      "                (in_proj): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (7-11): 5 x Block(\n",
      "              (norm): RMSNorm()\n",
      "              (mixer): Mamba2(\n",
      "                (in_proj): Linear(in_features=512, out_features=2320, bias=False)\n",
      "                (conv1d): Conv1d(1280, 1280, kernel_size=(4,), stride=(1,), padding=(3,), groups=1280)\n",
      "                (act): SiLU()\n",
      "                (norm): RMSNorm()\n",
      "                (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (norm_f): RMSNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls): BertOnlyMLMHead(\n",
      "      (predictions): BertLMPredictionHead(\n",
      "        (transform): BertPredictionHeadTransform(\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (transform_act_fn): SiLU()\n",
      "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (decoder): Linear(in_features=512, out_features=40990, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (cls4value_rec): ReconstructionDistributionHead(\n",
      "      (param_proj): Sequential(\n",
      "        (cat_fc): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (cat_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (cat_gelu): QuickGELU()\n",
      "        (cat_proj): Linear(in_features=512, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (cls4clsGeneAlign): ReconstructionDistributionHead(\n",
      "      (param_proj): Sequential(\n",
      "        (cat_fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (cat_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (cat_gelu): QuickGELU()\n",
      "        (cat_proj): Linear(in_features=512, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (kl_enc): KL_Encoder(\n",
      "      (mu_enc): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (var_enc): Linear(in_features=512, out_features=256, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (rna_cell_projection): Sequential(\n",
      "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (rna_gene_projection): Sequential(\n",
      "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (encoder_atac): BertForMaskedLMWithATAC(\n",
      "    (bert): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(3072283, 512, padding_idx=0)\n",
      "        (values_embeddings): ContinuousValueEncoder(\n",
      "          (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
      "          (activation): ReLU()\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (concat_embeddings): Sequential(\n",
      "          (cat_fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      "          (cat_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (cat_gelu): QuickGELU()\n",
      "          (cat_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (position_embeddings): Embedding(32771, 512)\n",
      "        (token_type_embeddings): Embedding(2, 512)\n",
      "        (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (species_embeddings): Embedding(2, 512)\n",
      "        (modality_embeddings): Embedding(2, 512)\n",
      "      )\n",
      "      (encoder): MambaEncoder(\n",
      "        (backbone): MixerModel_new(\n",
      "          (layers): ModuleList(\n",
      "            (0-5): 6 x Block(\n",
      "              (norm): RMSNorm()\n",
      "              (mixer): Mamba2(\n",
      "                (in_proj): Linear(in_features=512, out_features=2320, bias=False)\n",
      "                (conv1d): Conv1d(1280, 1280, kernel_size=(4,), stride=(1,), padding=(3,), groups=1280)\n",
      "                (act): SiLU()\n",
      "                (norm): RMSNorm()\n",
      "                (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (6): Block(\n",
      "              (norm): RMSNorm()\n",
      "              (mixer): MHA(\n",
      "                (in_proj): Linear(in_features=512, out_features=1536, bias=True)\n",
      "                (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "            )\n",
      "            (7-11): 5 x Block(\n",
      "              (norm): RMSNorm()\n",
      "              (mixer): Mamba2(\n",
      "                (in_proj): Linear(in_features=512, out_features=2320, bias=False)\n",
      "                (conv1d): Conv1d(1280, 1280, kernel_size=(4,), stride=(1,), padding=(3,), groups=1280)\n",
      "                (act): SiLU()\n",
      "                (norm): RMSNorm()\n",
      "                (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (norm_f): RMSNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cls): BertOnlyMLMHead(\n",
      "      (predictions): BertLMPredictionHead(\n",
      "        (transform): BertPredictionHeadTransform(\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (transform_act_fn): SiLU()\n",
      "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (decoder): Linear(in_features=512, out_features=40990, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (cls4peak_acc): BertOnlyMLMHead(\n",
      "      (predictions): BertLMPredictionHead(\n",
      "        (transform): BertPredictionHeadTransform(\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (transform_act_fn): SiLU()\n",
      "          (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (decoder): Linear(in_features=512, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (cls4clsPeakAlign): Sequential(\n",
      "      (cat_fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (cat_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (cat_gelu): QuickGELU()\n",
      "      (cat_proj): Linear(in_features=512, out_features=1, bias=True)\n",
      "    )\n",
      "    (cls4clsPeakIDFAlign): Sequential(\n",
      "      (cat_fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (cat_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (cat_gelu): QuickGELU()\n",
      "      (cat_proj): Linear(in_features=512, out_features=1, bias=True)\n",
      "    )\n",
      "    (kl_enc): KL_Encoder(\n",
      "      (mu_enc): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (var_enc): Linear(in_features=512, out_features=256, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (atac_cell_projection): Sequential(\n",
      "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (atac_gene_projection): Sequential(\n",
      "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (ot_loss): OptimalTransportLoss(\n",
      "    (loss): SamplesLoss()\n",
      "  )\n",
      "  (loss_R2R): ReconstructionDistributionHead(\n",
      "    (param_proj): Sequential(\n",
      "      (cat_fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (cat_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (cat_gelu): QuickGELU()\n",
      "      (cat_proj): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "dict_keys(['model', 'args', 'example_lengths_file', 'token_dictionary', 'data_collator'])\n",
      "[2025-08-26 14:18:35,284] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/public/share/t_lgl/scFM/shared/Final_models_250514/SCARF/conda_scarf/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loaded_state_dict = load_model_with_index(checkpoint_path)\n",
    "with open(checkpoint_path + '/config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "loaded_config = BertConfig(**config)\n",
    "loaded_config[\"atac_model_cfg\"][\"x_peak_inacc_ratio\"] = 0\n",
    "\n",
    "model = TotalModel_downstream(loaded_config, priors=priors)\n",
    "model.load_state_dict(loaded_state_dict, strict=False)\n",
    "del loaded_state_dict\n",
    "print(model)\n",
    "\n",
    "batch_size = 16\n",
    "training_args = {\n",
    "    \"per_device_eval_batch_size\": batch_size,\n",
    "    \"output_dir\": output_dir,\n",
    "    \"half_precision_backend\": 'apex',\n",
    "    \"dataloader_num_workers\": 8,\n",
    "    \"dataloader_prefetch_factor\": 2,\n",
    "    \"do_train\": False,\n",
    "    \"do_eval\": True,\n",
    "    \"group_by_length\": False,\n",
    "    \"length_column_name\": \"length\",\n",
    "    \"disable_tqdm\": False,\n",
    "    \"save_safetensors\": False\n",
    "}\n",
    "\n",
    "training_args = TrainingArguments(**training_args)\n",
    "trainer = Pretrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    example_lengths_file=sorted_len,\n",
    "    token_dictionary=token_dictionary,\n",
    "    data_collator=DataCollatorForLanguageModeling_Inference(\n",
    "        tokenizer=PreCollator(token_dictionary=token_dictionary),\n",
    "        mlm=True, config=loaded_config, priors=priors,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5. Outputs of SCARF\n",
    "For each run, the script generates:\n",
    "\n",
    "Cell embeddings (RNA and/or ATAC)\n",
    "\n",
    "Cell names matching the embeddings\n",
    "\n",
    "Label files (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/602 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 602/602 [03:30<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9631 (0,) 0\n",
      "9631 (0,) 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 602/602 [03:28<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9631 (0,) 0\n",
      "9631 (0,) 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dataloader = trainer.get_test_dataloader(dataset)\n",
    "trainer.model.eval()\n",
    "\n",
    "is_only_emb = True\n",
    "direct_outs = [False]\n",
    "for direct_out in direct_outs:\n",
    "    rna_res, atac_res, cell_names = [], [], []\n",
    "    for step, inputs in enumerate(tqdm(test_dataloader)):\n",
    "        with torch.no_grad():\n",
    "            inputs_new = {}\n",
    "            if args.embed_type == 'RNA':\n",
    "                to_cuda_names = ['rna_gene_ids', 'rna_gene_values', 'rna_attention_mask', 'species', 'modality']\n",
    "            elif args.embed_type == 'ATAC':\n",
    "                to_cuda_names = ['atac_cell_peaks','atac_peak_ids', 'atac_peak_idfs','atac_attention_mask', 'species', 'modality']\n",
    "            else:\n",
    "                to_cuda_names = ['rna_gene_ids', 'rna_gene_values', 'rna_attention_mask','atac_peak_ids', 'atac_peak_idfs',\n",
    "                                 'atac_attention_mask', 'species', 'modality']\n",
    "\n",
    "            for each_name in to_cuda_names:\n",
    "                if each_name in inputs:\n",
    "                    inputs_new[each_name] = inputs[each_name].to(\"cuda\")\n",
    "            inputs_new['cell_name'] = inputs['cell_name']\n",
    "            inputs_new['direct_out'] = direct_out\n",
    "            if args.embed_type == 'RNA_and_ATAC':\n",
    "                rna_res_dict, atac_res_dict, pred, gt = trainer.model.match_forward(**inputs_new)\n",
    "                rna_res.extend(list(rna_res_dict.values()))\n",
    "                atac_res.extend(list(atac_res_dict.values()))\n",
    "                cell_names.extend(list(rna_res_dict.keys()))\n",
    "                del rna_res_dict, atac_res_dict, pred, gt\n",
    "            elif args.embed_type == 'RNA':\n",
    "                rna_res_dict = trainer.model.get_rna_embeddings(**inputs_new)\n",
    "                rna_res.extend(list(rna_res_dict.values()))\n",
    "                cell_names.extend(list(rna_res_dict.keys()))\n",
    "                del rna_res_dict\n",
    "            elif args.embed_type == 'ATAC':\n",
    "                atac_res_dict = trainer.model.get_atac_embeddings(**inputs_new)\n",
    "                atac_res.extend(list(atac_res_dict.values()))\n",
    "                cell_names.extend(list(atac_res_dict.keys()))\n",
    "                del atac_res_dict\n",
    "    save_path = f\"{output_dir}/{dset_name}_{args.embed_type}_bs{batch_size}_directOut{int(direct_out)}\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    if 'RNA' in args.embed_type:\n",
    "        rna_res = rna_res[:data_len]\n",
    "        cell_names = cell_names[:data_len]\n",
    "        print(data_len, np.array(rna_res).shape, len(cell_names))\n",
    "        np.save(f'{save_path}/rna_cell_embs.npy', np.array(rna_res).astype(np.float32))\n",
    "        np.save(f'{save_path}/cell_names.npy', np.array(cell_names))\n",
    "\n",
    "    if 'ATAC' in args.embed_type:\n",
    "        atac_res = atac_res[:data_len]\n",
    "        cell_names = cell_names[:data_len]\n",
    "        print(data_len, np.array(atac_res).shape, len(cell_names))\n",
    "        np.save(f'{save_path}/atac_cell_embs.npy', np.array(atac_res).astype(np.float32))\n",
    "        np.save(f'{save_path}/cell_names.npy', np.array(cell_names))\n",
    "\n",
    "    if args.has_label:\n",
    "        cell_names = dataset['cell_name']\n",
    "        cell_types = dataset['cell_types']\n",
    "        name2type_dict = {cell_name: [cell_type] for cell_name, cell_type in zip(cell_names, cell_types)}\n",
    "        name2type_df = pd.DataFrame(name2type_dict).T\n",
    "        name2type_df.to_csv(f'{save_path}/labels.tsv.gz', header=False, index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}