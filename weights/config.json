{
  "architectures": [
    "TotalLossModel"
  ],
  "atac_model_cfg": {
    "atac_max_input_size": 4096,
    "atac_mlm_probability": 0.15,
    "atac_pos_max_size": 32768,
    "atac_use_emb": false,
    "attention_probs_dropout_prob": 0.02,
    "attn_cfg": {
      "num_heads": 8
    },
    "attn_layer_idx": [
      6
    ],
    "block_num": 3074,
    "d_intermediate": 0,
    "d_model": 512,
    "encoder_type": "mamba",
    "expand": 2,
    "fused_add_norm": true,
    "head_dim": 64,
    "hidden_act": "silu",
    "hidden_dropout_prob": 0.02,
    "hidden_size": 512,
    "inidtializer_range": 0.02,
    "intermediate_size": 1024,
    "kl_dim_atac": 256,
    "layer_norm_eps": 1e-12,
    "mamba_version": "v_manual",
    "max_position_embeddings": 8193,
    "n_groups": 8,
    "n_layer": 12,
    "num_attention_heads": 4,
    "num_heads": 16,
    "num_hidden_layers": 12,
    "pad_token_id": 0,
    "pad_vocab_size_multiple": 16,
    "peak_total_num": 3072283,
    "per_block_length": 1000,
    "residual_in_fp32": true,
    "rms_norm": true,
    "score_bins": 201,
    "sel_peak_inacc_aug": 2,
    "ssm_cfg": {
      "layer": "Mamba2"
    },
    "state_size": 128,
    "tie_embeddings": true,
    "use_atac_cl": false,
    "use_random_sample": false,
    "vocab_size": 40990,
    "x_peak_inacc_ratio": 0.1
  },
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "cluster_factor": 0.7,
  "encoder_type": "RNA",
  "focal_alpha": 0.5,
  "focal_gamma": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "loss_weights": [
    1,
    10,
    1,
    100,
    10
  ],
  "max_input_size": 4096,
  "max_position_embeddings": 32771,
  "mlp_out_size": 512,
  "modality_number": 2,
  "mode": "train",
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "peak_dim": 3,
  "peak_total_num": 3072283,
  "position_embedding_type": "absolute",
  "pretrain_mode": "RNA_ATAC",
  "queue_alpha": 0.05,
  "queue_size": 2048,
  "queue_temp": 0.07,
  "rna_model_cfg": {
    "CTA_num": 164,
    "R2R_rec": "poisson",
    "attention_probs_dropout_prob": 0.02,
    "attn_cfg": {
      "num_heads": 8
    },
    "attn_layer_idx": [
      6
    ],
    "d_intermediate": 0,
    "d_model": 512,
    "encoder_type": "mamba",
    "expand": 2,
    "fused_add_norm": true,
    "head_dim": 64,
    "hidden_act": "silu",
    "hidden_dropout_prob": 0.02,
    "hidden_size": 512,
    "inidtializer_range": 0.02,
    "intermediate_size": 1024,
    "kl_dim_rna": 256,
    "layer_norm_eps": 1e-12,
    "mamba_version": "v_manual",
    "max_position_embeddings": 2049,
    "n_groups": 8,
    "n_layer": 12,
    "num_attention_heads": 4,
    "num_heads": 16,
    "num_hidden_layers": 12,
    "pad_token_id": 0,
    "pad_vocab_size_multiple": 16,
    "residual_in_fp32": true,
    "rms_norm": true,
    "rna_max_input_size": 4096,
    "rna_mlm_probability": 0.15,
    "rna_pos_max_size": 8192,
    "sel_gene_aug": 1,
    "ssm_cfg": {
      "layer": "Mamba2"
    },
    "state_size": 128,
    "tie_embeddings": true,
    "use_CTA": false,
    "use_cls_gene_align": true,
    "use_random_sample": false,
    "use_rna_cl": false,
    "vocab_size": 40990
  },
  "species_num": 2,
  "torch_dtype": "float32",
  "transformers_version": "4.46.3",
  "transition_loss_weights": [
    1,
    10,
    10
  ],
  "type_vocab_size": 2,
  "use_cache": true,
  "use_cell_align": true,
  "use_cls_token": true,
  "use_cluster_label": false,
  "use_gene_align": false,
  "use_middle_cls_token": false,
  "use_queue": false,
  "use_rna_homo": false,
  "use_transition": false,
  "use_value_emb": true,
  "use_values": true,
  "vocab_size": 30522,
  "warmup_steps": 0,
  "warmup_steps_start": 1000
}
